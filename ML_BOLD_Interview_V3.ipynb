{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# # # Machine Learning / Artificial Intelligence Tech Lead Interview\n",
    "\n",
    "Welcome to Bold Commerce! For this interview we'll jump right into a problem and try to solve it together using whichever tools you believe are most appropriate for the task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ## Problem Statement\n",
    "\n",
    "Suppose that we have a fairly new application being developed here at Bold Commerce. This new app is simply a mobile application that tracks the all of the users activity such as browsing and shopping activity.\n",
    "\n",
    "The app development team has asked you to take some data from the app and extract some meaning from it. They were also interested in trying to possibly predict user behavior based on the usage data in the data they provided. For any given user, they want to see if they can use a snapshot of what that person has done at this particular time and predict what they are doing.\n",
    "\n",
    "\n",
    "\n",
    "In this notebook, we would like to see how you would approach solving this problem. The data is provided in the `data` directory as `train.csv` and `test.csv`, so do what you will with this dataset to address the request of the app development team. Good luck and have fun!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard packages\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# feel free to add any additional packages you feel you may need\n",
    "#from sklearn import ...\n",
    "#import tensorflow as tf ...\n",
    "# Load libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('train.csv')\n",
    "test_data = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QUESTION\n",
    "\n",
    "- What are some considerations for the nature of the dataset that you can initially explore?\n",
    "- Can you show what you would like to explore regarding the dataset as presented?\n",
    "\n",
    "I will first see the how my data looks like using .head().I found a column mentionaing the ID and 561 features. In the later part, 'activity. I'm taking it as a classifier target variable.'I found that Then I checked for the nulls values . I did not found single null value. This is a good sign. Using the .dtypes , I checked the columns datatypes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>feature_3</th>\n",
       "      <th>feature_4</th>\n",
       "      <th>feature_5</th>\n",
       "      <th>feature_6</th>\n",
       "      <th>feature_7</th>\n",
       "      <th>feature_8</th>\n",
       "      <th>feature_9</th>\n",
       "      <th>...</th>\n",
       "      <th>feature_553</th>\n",
       "      <th>feature_554</th>\n",
       "      <th>feature_555</th>\n",
       "      <th>feature_556</th>\n",
       "      <th>feature_557</th>\n",
       "      <th>feature_558</th>\n",
       "      <th>feature_559</th>\n",
       "      <th>feature_560</th>\n",
       "      <th>feature_561</th>\n",
       "      <th>activity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3368.0</td>\n",
       "      <td>0.283</td>\n",
       "      <td>-0.026</td>\n",
       "      <td>-0.111</td>\n",
       "      <td>-0.374</td>\n",
       "      <td>-0.100</td>\n",
       "      <td>-0.203</td>\n",
       "      <td>-0.376</td>\n",
       "      <td>-0.150</td>\n",
       "      <td>-0.182</td>\n",
       "      <td>...</td>\n",
       "      <td>0.515</td>\n",
       "      <td>0.275</td>\n",
       "      <td>-0.220</td>\n",
       "      <td>0.670</td>\n",
       "      <td>0.889</td>\n",
       "      <td>0.464</td>\n",
       "      <td>-0.760</td>\n",
       "      <td>0.140</td>\n",
       "      <td>0.186</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3369.0</td>\n",
       "      <td>0.207</td>\n",
       "      <td>-0.017</td>\n",
       "      <td>-0.094</td>\n",
       "      <td>-0.325</td>\n",
       "      <td>-0.124</td>\n",
       "      <td>-0.217</td>\n",
       "      <td>-0.333</td>\n",
       "      <td>-0.174</td>\n",
       "      <td>-0.204</td>\n",
       "      <td>...</td>\n",
       "      <td>0.362</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.945</td>\n",
       "      <td>0.602</td>\n",
       "      <td>0.807</td>\n",
       "      <td>-0.112</td>\n",
       "      <td>-0.762</td>\n",
       "      <td>0.142</td>\n",
       "      <td>0.184</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3370.0</td>\n",
       "      <td>0.205</td>\n",
       "      <td>-0.016</td>\n",
       "      <td>-0.090</td>\n",
       "      <td>-0.335</td>\n",
       "      <td>-0.098</td>\n",
       "      <td>-0.219</td>\n",
       "      <td>-0.344</td>\n",
       "      <td>-0.149</td>\n",
       "      <td>-0.199</td>\n",
       "      <td>...</td>\n",
       "      <td>0.650</td>\n",
       "      <td>0.524</td>\n",
       "      <td>0.916</td>\n",
       "      <td>0.195</td>\n",
       "      <td>0.812</td>\n",
       "      <td>-0.262</td>\n",
       "      <td>-0.759</td>\n",
       "      <td>0.142</td>\n",
       "      <td>0.186</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3371.0</td>\n",
       "      <td>0.241</td>\n",
       "      <td>-0.031</td>\n",
       "      <td>-0.106</td>\n",
       "      <td>-0.326</td>\n",
       "      <td>-0.066</td>\n",
       "      <td>-0.199</td>\n",
       "      <td>-0.333</td>\n",
       "      <td>-0.106</td>\n",
       "      <td>-0.166</td>\n",
       "      <td>...</td>\n",
       "      <td>0.364</td>\n",
       "      <td>0.149</td>\n",
       "      <td>0.468</td>\n",
       "      <td>-0.686</td>\n",
       "      <td>0.432</td>\n",
       "      <td>-0.366</td>\n",
       "      <td>-0.757</td>\n",
       "      <td>0.142</td>\n",
       "      <td>0.187</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3372.0</td>\n",
       "      <td>0.270</td>\n",
       "      <td>-0.014</td>\n",
       "      <td>-0.100</td>\n",
       "      <td>-0.309</td>\n",
       "      <td>-0.017</td>\n",
       "      <td>-0.161</td>\n",
       "      <td>-0.314</td>\n",
       "      <td>-0.053</td>\n",
       "      <td>-0.117</td>\n",
       "      <td>...</td>\n",
       "      <td>0.258</td>\n",
       "      <td>-0.081</td>\n",
       "      <td>0.441</td>\n",
       "      <td>-0.794</td>\n",
       "      <td>0.709</td>\n",
       "      <td>-0.169</td>\n",
       "      <td>-0.749</td>\n",
       "      <td>0.145</td>\n",
       "      <td>0.192</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 563 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       ID  feature_1  feature_2  feature_3  feature_4  feature_5  feature_6  feature_7  feature_8  \\\n",
       "0  3368.0      0.283     -0.026     -0.111     -0.374     -0.100     -0.203     -0.376     -0.150   \n",
       "1  3369.0      0.207     -0.017     -0.094     -0.325     -0.124     -0.217     -0.333     -0.174   \n",
       "2  3370.0      0.205     -0.016     -0.090     -0.335     -0.098     -0.219     -0.344     -0.149   \n",
       "3  3371.0      0.241     -0.031     -0.106     -0.326     -0.066     -0.199     -0.333     -0.106   \n",
       "4  3372.0      0.270     -0.014     -0.100     -0.309     -0.017     -0.161     -0.314     -0.053   \n",
       "\n",
       "   feature_9  ...  feature_553  feature_554  feature_555  feature_556  feature_557  feature_558  \\\n",
       "0     -0.182  ...        0.515        0.275       -0.220        0.670        0.889        0.464   \n",
       "1     -0.204  ...        0.362        0.016        0.945        0.602        0.807       -0.112   \n",
       "2     -0.199  ...        0.650        0.524        0.916        0.195        0.812       -0.262   \n",
       "3     -0.166  ...        0.364        0.149        0.468       -0.686        0.432       -0.366   \n",
       "4     -0.117  ...        0.258       -0.081        0.441       -0.794        0.709       -0.169   \n",
       "\n",
       "   feature_559  feature_560  feature_561  activity  \n",
       "0       -0.760        0.140        0.186       1.0  \n",
       "1       -0.762        0.142        0.184       1.0  \n",
       "2       -0.759        0.142        0.186       1.0  \n",
       "3       -0.757        0.142        0.187       1.0  \n",
       "4       -0.749        0.145        0.192       1.0  \n",
       "\n",
       "[5 rows x 563 columns]"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#code\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>feature_3</th>\n",
       "      <th>feature_4</th>\n",
       "      <th>feature_5</th>\n",
       "      <th>feature_6</th>\n",
       "      <th>feature_7</th>\n",
       "      <th>feature_8</th>\n",
       "      <th>feature_9</th>\n",
       "      <th>...</th>\n",
       "      <th>feature_552</th>\n",
       "      <th>feature_553</th>\n",
       "      <th>feature_554</th>\n",
       "      <th>feature_555</th>\n",
       "      <th>feature_556</th>\n",
       "      <th>feature_557</th>\n",
       "      <th>feature_558</th>\n",
       "      <th>feature_559</th>\n",
       "      <th>feature_560</th>\n",
       "      <th>feature_561</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.289</td>\n",
       "      <td>-0.020</td>\n",
       "      <td>-0.133</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.983</td>\n",
       "      <td>-0.914</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.983</td>\n",
       "      <td>-0.924</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.074</td>\n",
       "      <td>-0.299</td>\n",
       "      <td>-0.710</td>\n",
       "      <td>-0.113</td>\n",
       "      <td>0.030</td>\n",
       "      <td>-0.465</td>\n",
       "      <td>-0.018</td>\n",
       "      <td>-0.841</td>\n",
       "      <td>0.180</td>\n",
       "      <td>-0.059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.278</td>\n",
       "      <td>-0.016</td>\n",
       "      <td>-0.124</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.975</td>\n",
       "      <td>-0.960</td>\n",
       "      <td>-0.999</td>\n",
       "      <td>-0.975</td>\n",
       "      <td>-0.958</td>\n",
       "      <td>...</td>\n",
       "      <td>0.158</td>\n",
       "      <td>-0.595</td>\n",
       "      <td>-0.861</td>\n",
       "      <td>0.053</td>\n",
       "      <td>-0.007</td>\n",
       "      <td>-0.733</td>\n",
       "      <td>0.704</td>\n",
       "      <td>-0.845</td>\n",
       "      <td>0.180</td>\n",
       "      <td>-0.054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.280</td>\n",
       "      <td>-0.019</td>\n",
       "      <td>-0.113</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.967</td>\n",
       "      <td>-0.979</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.964</td>\n",
       "      <td>-0.977</td>\n",
       "      <td>...</td>\n",
       "      <td>0.415</td>\n",
       "      <td>-0.391</td>\n",
       "      <td>-0.760</td>\n",
       "      <td>-0.119</td>\n",
       "      <td>0.178</td>\n",
       "      <td>0.101</td>\n",
       "      <td>0.809</td>\n",
       "      <td>-0.849</td>\n",
       "      <td>0.181</td>\n",
       "      <td>-0.049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.0</td>\n",
       "      <td>0.279</td>\n",
       "      <td>-0.026</td>\n",
       "      <td>-0.123</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.983</td>\n",
       "      <td>-0.991</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.983</td>\n",
       "      <td>-0.989</td>\n",
       "      <td>...</td>\n",
       "      <td>0.405</td>\n",
       "      <td>-0.117</td>\n",
       "      <td>-0.483</td>\n",
       "      <td>-0.037</td>\n",
       "      <td>-0.013</td>\n",
       "      <td>0.640</td>\n",
       "      <td>-0.485</td>\n",
       "      <td>-0.849</td>\n",
       "      <td>0.182</td>\n",
       "      <td>-0.048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>0.277</td>\n",
       "      <td>-0.017</td>\n",
       "      <td>-0.115</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.981</td>\n",
       "      <td>-0.990</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.980</td>\n",
       "      <td>-0.990</td>\n",
       "      <td>...</td>\n",
       "      <td>0.088</td>\n",
       "      <td>-0.351</td>\n",
       "      <td>-0.699</td>\n",
       "      <td>0.123</td>\n",
       "      <td>0.123</td>\n",
       "      <td>0.694</td>\n",
       "      <td>-0.616</td>\n",
       "      <td>-0.848</td>\n",
       "      <td>0.185</td>\n",
       "      <td>-0.044</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 562 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    ID  feature_1  feature_2  feature_3  feature_4  feature_5  feature_6  feature_7  feature_8  \\\n",
       "0  1.0      0.289     -0.020     -0.133     -0.995     -0.983     -0.914     -0.995     -0.983   \n",
       "1  2.0      0.278     -0.016     -0.124     -0.998     -0.975     -0.960     -0.999     -0.975   \n",
       "2  3.0      0.280     -0.019     -0.113     -0.995     -0.967     -0.979     -0.997     -0.964   \n",
       "3  4.0      0.279     -0.026     -0.123     -0.996     -0.983     -0.991     -0.997     -0.983   \n",
       "4  5.0      0.277     -0.017     -0.115     -0.998     -0.981     -0.990     -0.998     -0.980   \n",
       "\n",
       "   feature_9  ...  feature_552  feature_553  feature_554  feature_555  feature_556  feature_557  \\\n",
       "0     -0.924  ...       -0.074       -0.299       -0.710       -0.113        0.030       -0.465   \n",
       "1     -0.958  ...        0.158       -0.595       -0.861        0.053       -0.007       -0.733   \n",
       "2     -0.977  ...        0.415       -0.391       -0.760       -0.119        0.178        0.101   \n",
       "3     -0.989  ...        0.405       -0.117       -0.483       -0.037       -0.013        0.640   \n",
       "4     -0.990  ...        0.088       -0.351       -0.699        0.123        0.123        0.694   \n",
       "\n",
       "   feature_558  feature_559  feature_560  feature_561  \n",
       "0       -0.018       -0.841        0.180       -0.059  \n",
       "1        0.704       -0.845        0.180       -0.054  \n",
       "2        0.809       -0.849        0.181       -0.049  \n",
       "3       -0.485       -0.849        0.182       -0.048  \n",
       "4       -0.616       -0.848        0.185       -0.044  \n",
       "\n",
       "[5 rows x 562 columns]"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.head()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    " We must have a very good handle on how much data you have, both in terms of rows and columns.\n",
    " Too many rows and algorithms may take too long to train. Too few and perhaps you do not have enough data to train the algorithms.\n",
    " Too many features and some algorithms can be distracted or suffer poor performance due to the curse of dimensionality.\n",
    "We can review the shape and size of our dataset by printing the shape property on the Pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3675, 563)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID             0\n",
      "feature_1      0\n",
      "feature_2      0\n",
      "feature_3      0\n",
      "feature_4      0\n",
      "feature_5      0\n",
      "feature_6      0\n",
      "feature_7      0\n",
      "feature_8      0\n",
      "feature_9      0\n",
      "feature_10     0\n",
      "feature_11     0\n",
      "feature_12     0\n",
      "feature_13     0\n",
      "feature_14     0\n",
      "feature_15     0\n",
      "feature_16     0\n",
      "feature_17     0\n",
      "feature_18     0\n",
      "feature_19     0\n",
      "feature_20     0\n",
      "feature_21     0\n",
      "feature_22     0\n",
      "feature_23     0\n",
      "feature_24     0\n",
      "feature_25     0\n",
      "feature_26     0\n",
      "feature_27     0\n",
      "feature_28     0\n",
      "feature_29     0\n",
      "              ..\n",
      "feature_533    0\n",
      "feature_534    0\n",
      "feature_535    0\n",
      "feature_536    0\n",
      "feature_537    0\n",
      "feature_538    0\n",
      "feature_539    0\n",
      "feature_540    0\n",
      "feature_541    0\n",
      "feature_542    0\n",
      "feature_543    0\n",
      "feature_544    0\n",
      "feature_545    0\n",
      "feature_546    0\n",
      "feature_547    0\n",
      "feature_548    0\n",
      "feature_549    0\n",
      "feature_550    0\n",
      "feature_551    0\n",
      "feature_552    0\n",
      "feature_553    0\n",
      "feature_554    0\n",
      "feature_555    0\n",
      "feature_556    0\n",
      "feature_557    0\n",
      "feature_558    0\n",
      "feature_559    0\n",
      "feature_560    0\n",
      "feature_561    0\n",
      "activity       0\n",
      "Length: 563, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(train_data.isnull().sum())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The type of each attribute is important. Strings may need to be converted to floating point values or integers to represent categorical or ordinal values. \n",
    "We can list the data types used by the DataFrame to characterize each attribute using the dtypes property."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ID             float64\n",
       "feature_1      float64\n",
       "feature_2      float64\n",
       "feature_3      float64\n",
       "feature_4      float64\n",
       "feature_5      float64\n",
       "feature_6      float64\n",
       "feature_7      float64\n",
       "feature_8      float64\n",
       "feature_9      float64\n",
       "feature_10     float64\n",
       "feature_11     float64\n",
       "feature_12     float64\n",
       "feature_13     float64\n",
       "feature_14     float64\n",
       "feature_15     float64\n",
       "feature_16     float64\n",
       "feature_17     float64\n",
       "feature_18     float64\n",
       "feature_19     float64\n",
       "feature_20     float64\n",
       "feature_21     float64\n",
       "feature_22     float64\n",
       "feature_23     float64\n",
       "feature_24     float64\n",
       "feature_25     float64\n",
       "feature_26     float64\n",
       "feature_27     float64\n",
       "feature_28     float64\n",
       "feature_29     float64\n",
       "                ...   \n",
       "feature_533    float64\n",
       "feature_534    float64\n",
       "feature_535    float64\n",
       "feature_536    float64\n",
       "feature_537    float64\n",
       "feature_538    float64\n",
       "feature_539    float64\n",
       "feature_540    float64\n",
       "feature_541    float64\n",
       "feature_542    float64\n",
       "feature_543    float64\n",
       "feature_544    float64\n",
       "feature_545    float64\n",
       "feature_546    float64\n",
       "feature_547    float64\n",
       "feature_548    float64\n",
       "feature_549    float64\n",
       "feature_550    float64\n",
       "feature_551    float64\n",
       "feature_552    float64\n",
       "feature_553    float64\n",
       "feature_554    float64\n",
       "feature_555    float64\n",
       "feature_556    float64\n",
       "feature_557    float64\n",
       "feature_558    float64\n",
       "feature_559    float64\n",
       "feature_560    float64\n",
       "feature_561    float64\n",
       "activity       float64\n",
       "Length: 563, dtype: object"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.dtypes"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Descriptive Statistics:\n",
    "\n",
    "Descriptive statistics can give you great insight into the shape of each attribute. Often we can create more summaries than we have time to review. The describe() function on the Pandas DataFrame lists 8 statistical properties of each attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       feature_1  feature_2  feature_3  feature_4  feature_5  feature_6  feature_7  feature_8  \\\n",
      "count   3675.000   3675.000   3675.000   3675.000   3675.000   3675.000   3675.000   3675.000   \n",
      "mean       0.275     -0.018     -0.110     -0.605     -0.527     -0.609     -0.630     -0.541   \n",
      "std        0.067      0.049      0.064      0.452      0.486      0.406      0.429      0.472   \n",
      "min       -0.504     -1.000     -0.926     -1.000     -1.000     -1.000     -1.000     -0.999   \n",
      "25%        0.265     -0.025     -0.121     -0.993     -0.979     -0.981     -0.994     -0.979   \n",
      "50%        0.277     -0.017     -0.109     -0.949     -0.858     -0.865     -0.953     -0.867   \n",
      "75%        0.289     -0.012     -0.099     -0.231     -0.074     -0.257     -0.284     -0.107   \n",
      "max        1.000      1.000      1.000      1.000      0.916      0.730      1.000      0.968   \n",
      "\n",
      "       feature_9  feature_10  ...  feature_553  feature_554  feature_555  feature_556  \\\n",
      "count   3675.000    3675.000  ...     3675.000     3675.000     3675.000     3675.000   \n",
      "mean      -0.612      -0.465  ...       -0.296       -0.615        0.008        0.009   \n",
      "std        0.401       0.548  ...        0.325        0.311        0.331        0.447   \n",
      "min       -1.000      -1.000  ...       -0.995       -1.000       -0.976       -1.000   \n",
      "25%       -0.981      -0.936  ...       -0.538       -0.840       -0.121       -0.281   \n",
      "50%       -0.860      -0.888  ...       -0.329       -0.699        0.009        0.014   \n",
      "75%       -0.266       0.008  ...       -0.107       -0.486        0.146        0.297   \n",
      "max        0.681       1.000  ...        0.941        0.927        0.981        1.000   \n",
      "\n",
      "       feature_557  feature_558  feature_559  feature_560  feature_561  activity  \n",
      "count     3675.000     3675.000     3675.000     3675.000     3675.000  3675.000  \n",
      "mean         0.011       -0.005       -0.491        0.068       -0.081     3.643  \n",
      "std          0.607        0.478        0.531        0.266        0.286     1.745  \n",
      "min         -0.996       -0.995       -0.999       -0.949       -1.000     1.000  \n",
      "25%         -0.484       -0.373       -0.825       -0.029       -0.194     2.000  \n",
      "50%          0.010        0.003       -0.740        0.177       -0.012     4.000  \n",
      "75%          0.511        0.361       -0.539        0.241        0.092     5.000  \n",
      "max          0.999        0.991        1.000        0.478        1.000     6.000  \n",
      "\n",
      "[8 rows x 562 columns]\n"
     ]
    }
   ],
   "source": [
    "# Statistical Summary\n",
    "from pandas import read_csv\n",
    "from pandas import set_option\n",
    "filename = \"train_data.csv\"\n",
    "set_option('display.width', 100)\n",
    "set_option('precision', 3)\n",
    "description = train_data.drop('ID', axis = 1).describe()\n",
    "print(description)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "##Class Distribution:\n",
    "On classiffication problems we need to know how balanced the class values are. Highly imbalanced problems (a lot more observations for one class than another) are common and may need special handling in the data preparation stage of our project. we can quickly get an idea of the distribution of the class attribute in Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "activity\n",
      "1.0    613\n",
      "2.0    536\n",
      "3.0    493\n",
      "4.0    643\n",
      "5.0    687\n",
      "6.0    703\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "class_counts = train_data.groupby('activity').size()\n",
    "print(class_counts)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    " ##Correlations Between Attributes:\n",
    "Correlation refers to the relationship between two variables and how they may or may not change together. The most common method for calculating correlation is Pearson's Correlation Coefficient, that assumes a normal distribution of the attributes involved. A correlation of -1 or 1 shows a full negative or positive correlation respectively. Whereas a value of 0 shows no\n",
    "correlation at all. Some machine learning algorithms like linear and logistic regression can suffer poor performance if there are highly correlated attributes in our dataset. As such, it is a good idea to review all of the pairwise correlations of the attributes in our dataset. We can use the corr() function on the Pandas DataFrame to calculate a correlation matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             feature_1  feature_2  feature_3  feature_4  feature_5  feature_6  feature_7  \\\n",
      "feature_1    1.000e+00  2.780e-01     -0.361      0.009     -0.018     -0.036      0.015   \n",
      "feature_2    2.780e-01  1.000e+00     -0.172     -0.041     -0.035     -0.048     -0.040   \n",
      "feature_3   -3.615e-01 -1.716e-01      1.000     -0.031     -0.035     -0.012     -0.030   \n",
      "feature_4    9.292e-03 -4.065e-02     -0.031      1.000      0.944      0.885      0.998   \n",
      "feature_5   -1.816e-02 -3.531e-02     -0.035      0.944      1.000      0.923      0.939   \n",
      "feature_6   -3.645e-02 -4.849e-02     -0.012      0.885      0.923      1.000      0.874   \n",
      "feature_7    1.484e-02 -4.000e-02     -0.030      0.998      0.939      0.874      1.000   \n",
      "feature_8   -1.955e-02 -3.519e-02     -0.035      0.937      0.997      0.917      0.932   \n",
      "feature_9   -4.010e-02 -5.079e-02     -0.003      0.873      0.917      0.997      0.862   \n",
      "feature_10   4.253e-02 -2.672e-02     -0.048      0.979      0.932      0.893      0.970   \n",
      "feature_11   2.355e-02  1.186e-01     -0.057      0.912      0.957      0.894      0.905   \n",
      "feature_12  -7.561e-02 -6.124e-02      0.102      0.868      0.883      0.943      0.859   \n",
      "feature_13   5.726e-02  6.107e-02      0.017     -0.970     -0.947     -0.903     -0.965   \n",
      "feature_14   3.862e-02  1.328e-01      0.013     -0.923     -0.952     -0.882     -0.920   \n",
      "feature_15  -2.243e-02  1.892e-02      0.127     -0.872     -0.892     -0.937     -0.864   \n",
      "feature_16  -2.416e-02 -4.023e-02     -0.017      0.974      0.978      0.942      0.971   \n",
      "feature_17   2.526e-02 -3.032e-02     -0.030      0.945      0.831      0.747      0.952   \n",
      "feature_18  -4.391e-02 -3.419e-02     -0.024      0.807      0.877      0.773      0.809   \n",
      "feature_19  -9.245e-02 -7.293e-02      0.069      0.748      0.781      0.923      0.738   \n",
      "feature_20   2.839e-02 -3.983e-02     -0.028      0.980      0.921      0.843      0.989   \n",
      "feature_21  -2.100e-02 -3.636e-02     -0.034      0.911      0.976      0.893      0.906   \n",
      "feature_22  -4.884e-02 -5.706e-02      0.017      0.834      0.888      0.968      0.824   \n",
      "feature_23   1.670e-01  2.193e-02     -0.140      0.792      0.848      0.824      0.785   \n",
      "feature_24   5.043e-02  2.136e-01     -0.038      0.820      0.877      0.831      0.814   \n",
      "feature_25  -1.238e-01 -4.552e-02      0.302      0.773      0.820      0.817      0.767   \n",
      "feature_26   2.454e-02  2.992e-02      0.002     -0.670     -0.719     -0.716     -0.661   \n",
      "feature_27  -7.193e-03 -3.706e-02      0.010      0.524      0.582      0.587      0.514   \n",
      "feature_28  -1.763e-02  4.048e-02     -0.020     -0.361     -0.373     -0.344     -0.359   \n",
      "feature_29   3.397e-02 -3.846e-02      0.003      0.353      0.303      0.237      0.356   \n",
      "feature_30   2.413e-02  3.451e-02      0.016     -0.495     -0.592     -0.561     -0.485   \n",
      "...                ...        ...        ...        ...        ...        ...        ...   \n",
      "feature_533 -1.031e-02 -9.279e-03     -0.032      0.604      0.600      0.641      0.600   \n",
      "feature_534  3.801e-04 -3.243e-02     -0.048      0.889      0.898      0.920      0.881   \n",
      "feature_535  2.334e-03 -2.306e-02     -0.066      0.730      0.722      0.773      0.722   \n",
      "feature_536  8.942e-03 -3.685e-02     -0.044      0.867      0.878      0.889      0.860   \n",
      "feature_537 -8.605e-03 -3.670e-02     -0.030      0.894      0.933      0.919      0.885   \n",
      "feature_538  4.239e-02 -1.617e-03      0.002      0.337      0.362      0.333      0.335   \n",
      "feature_539  4.438e-02 -8.398e-03     -0.007      0.202      0.165      0.172      0.207   \n",
      "feature_540 -3.142e-02  1.175e-03     -0.007     -0.213     -0.185     -0.148     -0.215   \n",
      "feature_541 -2.565e-02 -3.496e-03     -0.009     -0.220     -0.198     -0.156     -0.221   \n",
      "feature_542  1.164e-02 -2.436e-02     -0.035      0.821      0.820      0.860      0.816   \n",
      "feature_543  1.696e-02 -2.016e-02     -0.029      0.767      0.775      0.826      0.762   \n",
      "feature_544  1.528e-02 -2.184e-02     -0.032      0.785      0.788      0.840      0.780   \n",
      "feature_545  2.035e-02 -1.895e-02     -0.026      0.747      0.764      0.807      0.744   \n",
      "feature_546  7.927e-03 -1.558e-02     -0.016      0.697      0.688      0.716      0.693   \n",
      "feature_547  1.164e-02 -2.436e-02     -0.035      0.821      0.820      0.860      0.816   \n",
      "feature_548  1.798e-02 -1.062e-02     -0.021      0.508      0.497      0.598      0.506   \n",
      "feature_549  1.667e-02 -2.672e-02     -0.038      0.805      0.806      0.853      0.800   \n",
      "feature_550 -5.866e-03 -3.286e-02     -0.033      0.912      0.943      0.934      0.903   \n",
      "feature_551 -3.891e-03 -3.836e-03     -0.004      0.103      0.075      0.074      0.104   \n",
      "feature_552  1.136e-02 -2.789e-02     -0.019     -0.022     -0.063     -0.066     -0.013   \n",
      "feature_553 -1.151e-03 -1.132e-02      0.027      0.186      0.254      0.226      0.188   \n",
      "feature_554 -2.530e-04 -1.672e-02      0.024      0.153      0.223      0.189      0.156   \n",
      "feature_555 -5.659e-01  5.691e-02      0.038     -0.031     -0.022     -0.013     -0.037   \n",
      "feature_556  8.385e-03 -3.335e-04     -0.041     -0.029     -0.015     -0.024     -0.028   \n",
      "feature_557  3.673e-02  1.967e-02     -0.049      0.035      0.005     -0.001      0.037   \n",
      "feature_558  3.465e-02  8.605e-02     -0.035     -0.023     -0.017     -0.019     -0.024   \n",
      "feature_559 -1.376e-02  7.353e-03      0.018     -0.377     -0.387     -0.390     -0.371   \n",
      "feature_560  2.905e-02  1.900e-02     -0.056      0.491      0.545      0.504      0.486   \n",
      "feature_561 -4.686e-03 -2.400e-02     -0.014      0.411      0.449      0.463      0.406   \n",
      "activity     9.802e-03  2.707e-02      0.046     -0.725     -0.808     -0.807     -0.717   \n",
      "\n",
      "             feature_8  feature_9  feature_10  ...  feature_553  feature_554  feature_555  \\\n",
      "feature_1       -0.020     -0.040       0.043  ...       -0.001   -2.530e-04       -0.566   \n",
      "feature_2       -0.035     -0.051      -0.027  ...       -0.011   -1.672e-02        0.057   \n",
      "feature_3       -0.035     -0.003      -0.048  ...        0.027    2.393e-02        0.038   \n",
      "feature_4        0.937      0.873       0.979  ...        0.186    1.528e-01       -0.031   \n",
      "feature_5        0.997      0.917       0.932  ...        0.254    2.233e-01       -0.022   \n",
      "feature_6        0.917      0.997       0.893  ...        0.226    1.893e-01       -0.013   \n",
      "feature_7        0.932      0.862       0.970  ...        0.188    1.563e-01       -0.037   \n",
      "feature_8        1.000      0.910       0.925  ...        0.245    2.155e-01       -0.024   \n",
      "feature_9        0.910      1.000       0.881  ...        0.221    1.843e-01       -0.014   \n",
      "feature_10       0.925      0.881       1.000  ...        0.195    1.572e-01       -0.033   \n",
      "feature_11       0.953      0.889       0.907  ...        0.232    1.973e-01       -0.013   \n",
      "feature_12       0.875      0.934       0.869  ...        0.236    1.943e-01        0.003   \n",
      "feature_13      -0.940     -0.890      -0.947  ...       -0.213   -1.792e-01        0.010   \n",
      "feature_14      -0.934     -0.871      -0.907  ...       -0.267   -2.338e-01        0.018   \n",
      "feature_15      -0.883     -0.919      -0.881  ...       -0.227   -1.936e-01        0.019   \n",
      "feature_16       0.974      0.936       0.958  ...        0.212    1.790e-01       -0.021   \n",
      "feature_17       0.826      0.730       0.900  ...        0.107    8.047e-02       -0.048   \n",
      "feature_18       0.880      0.767       0.776  ...        0.164    1.463e-01       -0.023   \n",
      "feature_19       0.776      0.920       0.753  ...        0.139    1.077e-01       -0.015   \n",
      "feature_20       0.915      0.830       0.936  ...        0.191    1.639e-01       -0.056   \n",
      "feature_21       0.988      0.887       0.900  ...        0.225    1.966e-01       -0.030   \n",
      "feature_22       0.883      0.982       0.842  ...        0.208    1.735e-01       -0.014   \n",
      "feature_23       0.843      0.821       0.791  ...        0.323    2.858e-01       -0.073   \n",
      "feature_24       0.872      0.827       0.824  ...        0.329    2.864e-01        0.008   \n",
      "feature_25       0.815      0.814       0.772  ...        0.304    2.613e-01       -0.011   \n",
      "feature_26      -0.716     -0.721      -0.677  ...       -0.251   -2.077e-01        0.006   \n",
      "feature_27       0.579      0.590       0.541  ...        0.246    2.125e-01       -0.018   \n",
      "feature_28      -0.371     -0.345      -0.358  ...       -0.084   -6.974e-02        0.036   \n",
      "feature_29       0.301      0.233       0.331  ...       -0.062   -6.066e-02       -0.027   \n",
      "feature_30      -0.594     -0.568      -0.504  ...       -0.291   -2.435e-01        0.006   \n",
      "...                ...        ...         ...  ...          ...          ...          ...   \n",
      "feature_533      0.592      0.629       0.605  ...        0.180    1.443e-01        0.004   \n",
      "feature_534      0.887      0.907       0.894  ...        0.222    1.780e-01       -0.023   \n",
      "feature_535      0.711      0.763       0.735  ...        0.115    7.601e-02       -0.021   \n",
      "feature_536      0.865      0.875       0.871  ...        0.213    1.703e-01       -0.032   \n",
      "feature_537      0.925      0.913       0.899  ...        0.346    2.893e-01       -0.014   \n",
      "feature_538      0.363      0.329       0.353  ...        0.140    1.537e-01       -0.023   \n",
      "feature_539      0.158      0.152       0.200  ...        0.039    5.570e-02       -0.027   \n",
      "feature_540     -0.179     -0.135      -0.208  ...        0.084    7.984e-02        0.021   \n",
      "feature_541     -0.193     -0.144      -0.216  ...        0.066    6.647e-02        0.018   \n",
      "feature_542      0.806      0.839       0.825  ...        0.226    1.804e-01       -0.030   \n",
      "feature_543      0.757      0.806       0.775  ...        0.303    2.527e-01       -0.029   \n",
      "feature_544      0.771      0.820       0.793  ...        0.238    1.851e-01       -0.030   \n",
      "feature_545      0.746      0.787       0.757  ...        0.385    3.504e-01       -0.030   \n",
      "feature_546      0.677      0.697       0.697  ...        0.188    1.532e-01       -0.027   \n",
      "feature_547      0.806      0.839       0.825  ...        0.226    1.804e-01       -0.030   \n",
      "feature_548      0.479      0.575       0.510  ...        0.155    1.135e-01       -0.035   \n",
      "feature_549      0.792      0.834       0.810  ...        0.187    1.466e-01       -0.037   \n",
      "feature_550      0.934      0.925       0.918  ...        0.325    2.685e-01       -0.013   \n",
      "feature_551      0.077      0.075       0.098  ...       -0.273   -2.102e-01       -0.007   \n",
      "feature_552     -0.060     -0.072      -0.040  ...       -0.364   -2.255e-01       -0.018   \n",
      "feature_553      0.245      0.221       0.195  ...        1.000    9.674e-01        0.007   \n",
      "feature_554      0.215      0.184       0.157  ...        0.967    1.000e+00        0.006   \n",
      "feature_555     -0.024     -0.014      -0.033  ...        0.007    6.318e-03        1.000   \n",
      "feature_556     -0.012     -0.023      -0.030  ...        0.010    8.108e-03       -0.064   \n",
      "feature_557      0.004     -0.008       0.037  ...        0.032    2.198e-02        0.004   \n",
      "feature_558     -0.014     -0.020      -0.025  ...       -0.010   -1.034e-02       -0.013   \n",
      "feature_559     -0.381     -0.389      -0.395  ...       -0.106   -8.588e-02        0.010   \n",
      "feature_560      0.544      0.502       0.499  ...        0.160    1.388e-01       -0.002   \n",
      "feature_561      0.446      0.459       0.418  ...        0.104    8.698e-02        0.002   \n",
      "activity        -0.802     -0.807      -0.741  ...       -0.287   -2.650e-01       -0.004   \n",
      "\n",
      "             feature_556  feature_557  feature_558  feature_559  feature_560  feature_561  \\\n",
      "feature_1      8.385e-03    3.673e-02    3.465e-02       -0.014    2.905e-02       -0.005   \n",
      "feature_2     -3.335e-04    1.967e-02    8.605e-02        0.007    1.900e-02       -0.024   \n",
      "feature_3     -4.124e-02   -4.939e-02   -3.460e-02        0.018   -5.625e-02       -0.014   \n",
      "feature_4     -2.864e-02    3.499e-02   -2.300e-02       -0.377    4.907e-01        0.411   \n",
      "feature_5     -1.452e-02    5.339e-03   -1.650e-02       -0.387    5.452e-01        0.449   \n",
      "feature_6     -2.442e-02   -1.020e-03   -1.850e-02       -0.390    5.044e-01        0.463   \n",
      "feature_7     -2.827e-02    3.661e-02   -2.395e-02       -0.371    4.855e-01        0.406   \n",
      "feature_8     -1.187e-02    3.884e-03   -1.446e-02       -0.381    5.437e-01        0.446   \n",
      "feature_9     -2.344e-02   -7.622e-03   -2.006e-02       -0.389    5.016e-01        0.459   \n",
      "feature_10    -3.030e-02    3.654e-02   -2.461e-02       -0.395    4.985e-01        0.418   \n",
      "feature_11    -1.296e-02    5.812e-04    3.268e-04       -0.381    5.273e-01        0.433   \n",
      "feature_12    -3.526e-02   -1.904e-02   -6.136e-03       -0.386    4.675e-01        0.411   \n",
      "feature_13     2.952e-02   -2.185e-02    1.845e-02        0.374   -4.911e-01       -0.421   \n",
      "feature_14     2.349e-02   -2.089e-02    3.071e-02        0.384   -5.138e-01       -0.432   \n",
      "feature_15     2.602e-02   -4.756e-02    3.514e-02        0.369   -4.871e-01       -0.461   \n",
      "feature_16    -1.369e-02    1.525e-02   -1.925e-02       -0.371    5.078e-01        0.428   \n",
      "feature_17    -2.884e-02    5.539e-02   -2.987e-02       -0.324    4.090e-01        0.348   \n",
      "feature_18     1.731e-02   -2.800e-04   -2.142e-02       -0.235    4.277e-01        0.317   \n",
      "feature_19    -1.588e-02   -7.129e-03   -2.219e-02       -0.284    3.617e-01        0.396   \n",
      "feature_20    -2.958e-02    3.932e-02   -2.712e-02       -0.355    4.719e-01        0.394   \n",
      "feature_21    -6.720e-03    2.696e-03   -1.234e-02       -0.364    5.306e-01        0.433   \n",
      "feature_22    -2.597e-02   -1.669e-02   -2.696e-02       -0.380    4.863e-01        0.441   \n",
      "feature_23    -7.964e-03    4.732e-03    2.154e-03       -0.216    3.969e-01        0.293   \n",
      "feature_24    -1.599e-02    2.041e-02    1.097e-02       -0.429    5.691e-01        0.455   \n",
      "feature_25    -2.455e-02   -5.536e-04   -1.947e-02       -0.383    4.923e-01        0.441   \n",
      "feature_26    -1.745e-03    7.287e-04   -1.002e-03        0.101   -2.562e-01       -0.181   \n",
      "feature_27    -5.711e-03   -1.466e-02   -4.606e-03       -0.183    2.999e-01        0.227   \n",
      "feature_28     3.143e-03    1.551e-02    1.534e-03        0.030   -1.263e-01       -0.072   \n",
      "feature_29    -4.278e-03    2.946e-03    2.808e-03       -0.136    1.569e-01        0.130   \n",
      "feature_30     7.716e-03    1.842e-02   -1.681e-02        0.371   -4.757e-01       -0.364   \n",
      "...                  ...          ...          ...          ...          ...          ...   \n",
      "feature_533   -2.415e-02    4.653e-02   -6.363e-03       -0.229    2.992e-01        0.299   \n",
      "feature_534   -3.094e-02    2.656e-02   -2.319e-02       -0.380    4.757e-01        0.431   \n",
      "feature_535   -3.200e-02    2.830e-02   -2.073e-02       -0.306    3.593e-01        0.327   \n",
      "feature_536   -2.541e-02    2.138e-02   -2.487e-02       -0.377    4.686e-01        0.423   \n",
      "feature_537   -1.639e-02    1.697e-02   -7.833e-03       -0.411    5.502e-01        0.468   \n",
      "feature_538    5.898e-03   -2.104e-02   -1.125e-03       -0.188    2.157e-01        0.185   \n",
      "feature_539   -3.691e-02    1.720e-02   -3.394e-02        0.061   -5.393e-02        0.017   \n",
      "feature_540    2.793e-02    2.280e-02    5.162e-03        0.137   -1.538e-01       -0.189   \n",
      "feature_541    2.943e-02    2.471e-02    3.765e-03        0.167   -1.929e-01       -0.212   \n",
      "feature_542   -3.825e-02    4.043e-02   -3.424e-02       -0.340    4.252e-01        0.416   \n",
      "feature_543   -3.846e-02    4.164e-02   -3.723e-02       -0.332    4.044e-01        0.393   \n",
      "feature_544   -4.011e-02    4.063e-02   -3.630e-02       -0.336    4.103e-01        0.401   \n",
      "feature_545   -3.602e-02    4.325e-02   -3.795e-02       -0.326    4.010e-01        0.386   \n",
      "feature_546   -1.985e-02    3.435e-02   -1.102e-02       -0.280    3.550e-01        0.347   \n",
      "feature_547   -3.825e-02    4.043e-02   -3.424e-02       -0.340    4.252e-01        0.416   \n",
      "feature_548   -3.830e-02    4.561e-02   -3.682e-02       -0.202    2.272e-01        0.263   \n",
      "feature_549   -4.062e-02    2.934e-02   -3.753e-02       -0.339    4.205e-01        0.409   \n",
      "feature_550   -2.495e-02    2.279e-02   -1.438e-02       -0.403    5.380e-01        0.468   \n",
      "feature_551   -2.454e-02   -1.236e-03   -7.787e-03        0.004   -2.315e-02       -0.022   \n",
      "feature_552   -2.220e-02   -2.601e-02   -8.578e-03        0.131   -1.544e-01       -0.093   \n",
      "feature_553    1.031e-02    3.236e-02   -1.033e-02       -0.106    1.596e-01        0.104   \n",
      "feature_554    8.108e-03    2.198e-02   -1.034e-02       -0.086    1.388e-01        0.087   \n",
      "feature_555   -6.408e-02    3.828e-03   -1.273e-02        0.010   -2.008e-03        0.002   \n",
      "feature_556    1.000e+00   -9.120e-03    6.133e-02        0.027   -2.117e-02       -0.026   \n",
      "feature_557   -9.120e-03    1.000e+00   -6.937e-02       -0.006   -7.985e-03       -0.002   \n",
      "feature_558    6.133e-02   -6.937e-02    1.000e+00        0.017   -6.064e-04       -0.010   \n",
      "feature_559    2.660e-02   -6.152e-03    1.697e-02        1.000   -7.925e-01       -0.659   \n",
      "feature_560   -2.117e-02   -7.985e-03   -6.064e-04       -0.793    1.000e+00        0.786   \n",
      "feature_561   -2.578e-02   -1.802e-03   -9.541e-03       -0.659    7.860e-01        1.000   \n",
      "activity       1.950e-02    1.033e-02    1.678e-02        0.627   -6.424e-01       -0.543   \n",
      "\n",
      "             activity  \n",
      "feature_1       0.010  \n",
      "feature_2       0.027  \n",
      "feature_3       0.046  \n",
      "feature_4      -0.725  \n",
      "feature_5      -0.808  \n",
      "feature_6      -0.807  \n",
      "feature_7      -0.717  \n",
      "feature_8      -0.802  \n",
      "feature_9      -0.807  \n",
      "feature_10     -0.741  \n",
      "feature_11     -0.775  \n",
      "feature_12     -0.771  \n",
      "feature_13      0.758  \n",
      "feature_14      0.773  \n",
      "feature_15      0.751  \n",
      "feature_16     -0.771  \n",
      "feature_17     -0.536  \n",
      "feature_18     -0.596  \n",
      "feature_19     -0.637  \n",
      "feature_20     -0.700  \n",
      "feature_21     -0.780  \n",
      "feature_22     -0.788  \n",
      "feature_23     -0.746  \n",
      "feature_24     -0.765  \n",
      "feature_25     -0.714  \n",
      "feature_26      0.604  \n",
      "feature_27     -0.541  \n",
      "feature_28      0.278  \n",
      "feature_29     -0.170  \n",
      "feature_30      0.526  \n",
      "...               ...  \n",
      "feature_533    -0.473  \n",
      "feature_534    -0.738  \n",
      "feature_535    -0.558  \n",
      "feature_536    -0.735  \n",
      "feature_537    -0.804  \n",
      "feature_538    -0.456  \n",
      "feature_539    -0.179  \n",
      "feature_540     0.187  \n",
      "feature_541     0.206  \n",
      "feature_542    -0.676  \n",
      "feature_543    -0.673  \n",
      "feature_544    -0.669  \n",
      "feature_545    -0.681  \n",
      "feature_546    -0.550  \n",
      "feature_547    -0.676  \n",
      "feature_548    -0.391  \n",
      "feature_549    -0.669  \n",
      "feature_550    -0.815  \n",
      "feature_551    -0.073  \n",
      "feature_552     0.051  \n",
      "feature_553    -0.287  \n",
      "feature_554    -0.265  \n",
      "feature_555    -0.004  \n",
      "feature_556     0.020  \n",
      "feature_557     0.010  \n",
      "feature_558     0.017  \n",
      "feature_559     0.627  \n",
      "feature_560    -0.642  \n",
      "feature_561    -0.543  \n",
      "activity        1.000  \n",
      "\n",
      "[562 rows x 562 columns]\n"
     ]
    }
   ],
   "source": [
    "set_option('display.width', 100)\n",
    "set_option('precision', 3)\n",
    "correlations = train_data.drop('ID', axis = 1).corr(method='pearson')\n",
    "print(correlations)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The matrix lists all attributes across the top and down the side, to give correlation between all pairs of attributes (twice, because the matrix is symmetrical). You can see the diagonal line through the matrix from the top left to bottom right corners of the matrix shows perfect correlation of each attribute with itself."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "##Skew of Univariate Distributions:\n",
    "\n",
    "Skew refers to a distribution that is assumed Gaussian (normal or bell curve) that is shifted or squashed in one direction or another. Many machine learning algorithms assume a Gaussian distribution. Knowing that an attribute has a skew may allow us to perform data preparation to correct the skew and later improve the accuracy of our models. We can calculate the skew of each attribute using the skew() function on the Pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID            -0.027\n",
      "feature_1     -1.508\n",
      "feature_2      0.041\n",
      "feature_3      2.787\n",
      "feature_4      0.703\n",
      "feature_5      0.394\n",
      "feature_6      0.529\n",
      "feature_7      0.777\n",
      "feature_8      0.424\n",
      "feature_9      0.520\n",
      "feature_10     0.607\n",
      "feature_11     0.516\n",
      "feature_12     0.772\n",
      "feature_13    -0.627\n",
      "feature_14    -0.650\n",
      "feature_15    -0.957\n",
      "feature_16     0.453\n",
      "feature_17     1.917\n",
      "feature_18     3.043\n",
      "feature_19     1.899\n",
      "feature_20     1.005\n",
      "feature_21     0.571\n",
      "feature_22     0.617\n",
      "feature_23    -0.011\n",
      "feature_24    -0.066\n",
      "feature_25     0.006\n",
      "feature_26     0.083\n",
      "feature_27     0.457\n",
      "feature_28    -0.281\n",
      "feature_29    -0.074\n",
      "               ...  \n",
      "feature_533    3.054\n",
      "feature_534    1.082\n",
      "feature_535    2.818\n",
      "feature_536    1.121\n",
      "feature_537    0.006\n",
      "feature_538    2.094\n",
      "feature_539    0.152\n",
      "feature_540    0.758\n",
      "feature_541    1.429\n",
      "feature_542    1.823\n",
      "feature_543    2.114\n",
      "feature_544    2.053\n",
      "feature_545    2.071\n",
      "feature_546    2.557\n",
      "feature_547    1.823\n",
      "feature_548    5.281\n",
      "feature_549    1.943\n",
      "feature_550    0.180\n",
      "feature_551    5.125\n",
      "feature_552   -0.411\n",
      "feature_553    0.658\n",
      "feature_554    1.473\n",
      "feature_555   -0.047\n",
      "feature_556   -0.018\n",
      "feature_557   -0.017\n",
      "feature_558   -0.003\n",
      "feature_559    1.424\n",
      "feature_560   -1.290\n",
      "feature_561   -0.518\n",
      "activity      -0.148\n",
      "Length: 563, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "skew = train_data.skew()\n",
    "print(skew)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The skew result show a positive (right) or negative (left) skew. Values closer to zero show less skew."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QUESTION\n",
    "What algorithms would be appropriate for the provided problem and why?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "I'll apply the various classifiers algorithms with choosing significant features since by using coorelation. Selecting few features on the basis of +ve and -ve coorelation will make our algorithm much faster and will not overfit the data in comparison to selecting all the features. Choices of Algorithms:\n",
    "1.Support Vector Machine\n",
    "2.K-Nearest Neighbor\n",
    "3.Decision tree\n",
    "4.Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "feature_105   -0.867\n",
       "feature_103   -0.862\n",
       "feature_104   -0.856\n",
       "feature_368   -0.853\n",
       "feature_235   -0.851\n",
       "feature_185   -0.849\n",
       "feature_367   -0.847\n",
       "feature_369   -0.842\n",
       "feature_289   -0.839\n",
       "feature_290   -0.835\n",
       "feature_429   -0.832\n",
       "feature_126   -0.832\n",
       "feature_448   -0.832\n",
       "feature_524   -0.830\n",
       "feature_129   -0.826\n",
       "feature_288   -0.825\n",
       "feature_432   -0.825\n",
       "feature_261   -0.817\n",
       "feature_550   -0.815\n",
       "feature_88    -0.815\n",
       "feature_447   -0.814\n",
       "feature_273   -0.813\n",
       "feature_349   -0.812\n",
       "feature_426   -0.810\n",
       "feature_101   -0.810\n",
       "feature_85    -0.809\n",
       "feature_267   -0.809\n",
       "feature_435   -0.808\n",
       "feature_5     -0.808\n",
       "feature_352   -0.808\n",
       "               ...  \n",
       "feature_52     0.553\n",
       "feature_43     0.557\n",
       "feature_55     0.558\n",
       "feature_375    0.561\n",
       "feature_34     0.586\n",
       "feature_59     0.598\n",
       "feature_114    0.600\n",
       "feature_174    0.603\n",
       "feature_26     0.604\n",
       "feature_156    0.623\n",
       "feature_559    0.627\n",
       "feature_373    0.633\n",
       "feature_106    0.633\n",
       "feature_54     0.643\n",
       "feature_51     0.646\n",
       "feature_42     0.646\n",
       "feature_133    0.654\n",
       "feature_154    0.665\n",
       "feature_134    0.681\n",
       "feature_194    0.699\n",
       "feature_173    0.730\n",
       "feature_175    0.743\n",
       "feature_95     0.745\n",
       "feature_15     0.751\n",
       "feature_13     0.758\n",
       "feature_14     0.773\n",
       "feature_94     0.785\n",
       "feature_93     0.786\n",
       "feature_135    0.792\n",
       "activity       1.000\n",
       "Name: activity, Length: 562, dtype: float64"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Finding the coorelation \n",
    "train_data[train_data.columns[1:]].corr()['activity'][:].sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CODE\n",
    "#Transforming out train and test data according to our need!\n",
    "train_data1=train_data[['ID','feature_10','feature_103','feature_104','feature_368','feature_235','feature_185'\n",
    "                      ,'feature_367' ,'feature_369','feature_289','feature_290','feature_429','feature_126'\n",
    "                      ,'feature_448','feature_524','feature_129','feature_288','feature_432','feature_261'\n",
    "                      ,'feature_550','feature_88','feature_447','feature_273','feature_349','feature_426'\n",
    "                      ,'feature_101','feature_85','feature_267','feature_435','feature_5','feature_352'\n",
    "                      ,'feature_52','feature_43','feature_55','feature_375','feature_34','feature_59','feature_114'    \n",
    "                      ,'feature_174','feature_26','feature_156','feature_559','feature_373'    \n",
    "                      ,'feature_106','feature_54','feature_51', 'feature_42','feature_133','feature_154',\n",
    "                      'feature_134','feature_194' ,'feature_173','feature_175','feature_95',  \n",
    "                      'feature_15','feature_13' ,'feature_14' ,'feature_94','feature_93',\n",
    "                       'feature_135','activity']]\n",
    "test_data1=test_data[['ID','feature_10','feature_103','feature_104','feature_368','feature_235','feature_185'\n",
    "                      ,'feature_367' ,'feature_369','feature_289','feature_290','feature_429','feature_126'\n",
    "                      ,'feature_448','feature_524','feature_129','feature_288','feature_432','feature_261'\n",
    "                      ,'feature_550','feature_88','feature_447','feature_273','feature_349','feature_426'\n",
    "                      ,'feature_101','feature_85','feature_267','feature_435','feature_5','feature_352'\n",
    "                      ,'feature_52','feature_43','feature_55','feature_375','feature_34','feature_59','feature_114'    \n",
    "                      ,'feature_174','feature_26','feature_156','feature_559','feature_373'    \n",
    "                      ,'feature_106','feature_54','feature_51', 'feature_42','feature_133','feature_154',\n",
    "                      'feature_134','feature_194' ,'feature_173','feature_175','feature_95',  \n",
    "                      'feature_15','feature_13' ,'feature_14' ,'feature_94','feature_93',\n",
    "                       'feature_135']]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QUESTION\n",
    "\n",
    "- What results can be drawn from your exploration and modelling process?\n",
    "- What is your ultimate choice with regard to final model and why?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "Now I'll apply the four algorithms to get the best algorithm on the basis of accuracy of the algorithms! Choosing few features out of 562 features is far better as it will be good for space time tradeoff and will deliver the result.\n",
    "I'll be selecting the model on the basis of accuracy of models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "predictors=train_data1.drop(['ID','activity'],axis=1)\n",
    "target=train_data1['activity']\n",
    "seed = 6\n",
    "x_train,x_cv,y_train,y_cv=train_test_split(predictors,target,test_size=0.27,random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96.27\n",
      "confusion matrix:\n",
      "[[171   3   3   0   0   0]\n",
      " [  0 144   1   0   0   0]\n",
      " [  1   1 120   0   0   0]\n",
      " [  0   0   0 151   5   0]\n",
      " [  0   0   0  23 174   0]\n",
      " [  0   0   0   0   0 196]]\n"
     ]
    }
   ],
   "source": [
    "#KNN\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "knn = KNeighborsClassifier()\n",
    "knn.fit(x_train, y_train)\n",
    "y_pred = knn.predict(x_cv)\n",
    "acc_knn = round(accuracy_score(y_pred,y_cv) * 100, 2)\n",
    "print(acc_knn)\n",
    "#confusion matrix: \n",
    "matrix = confusion_matrix(y_pred, y_cv)\n",
    "print(\"confusion matrix:\")\n",
    "print(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hamroun\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93.66\n",
      "confusion matrix:\n",
      "[[169   7   4   0   0   0]\n",
      " [  1 139   2   0   0   0]\n",
      " [  2   2 118   0   0   0]\n",
      " [  0   0   0 132   3   0]\n",
      " [  0   0   0  42 176   0]\n",
      " [  0   0   0   0   0 196]]\n"
     ]
    }
   ],
   "source": [
    "#Support Vector Machine\n",
    "from sklearn.svm import SVC\n",
    "svc = SVC()\n",
    "svc.fit(x_train, y_train)\n",
    "y_pred = svc.predict(x_cv)\n",
    "acc_svc = round(accuracy_score(y_pred, y_cv) * 100, 2)\n",
    "print(acc_svc)\n",
    "#confusion matrix: \n",
    "matrix = confusion_matrix(y_pred, y_cv)\n",
    "print(\"confusion matrix:\")\n",
    "print(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75.93\n",
      "confusion matrix:\n",
      "[[123  19  15   0   0   0]\n",
      " [ 26 117  13   0   1   0]\n",
      " [ 23  12  96   0   0   0]\n",
      " [  0   0   0 149 105   0]\n",
      " [  0   0   0  21  73   0]\n",
      " [  0   0   0   4   0 196]]\n"
     ]
    }
   ],
   "source": [
    "#NaiveBaiyes\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "nb=GaussianNB()\n",
    "nb.fit(x_train, y_train)\n",
    "y_pred = nb.predict(x_cv)\n",
    "acc_nb = round(accuracy_score(y_pred, y_cv) * 100, 2)\n",
    "print(acc_nb)\n",
    "#confusion matrix: \n",
    "matrix = confusion_matrix(y_pred, y_cv)\n",
    "print(\"confusion matrix:\")\n",
    "print(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95.27\n",
      "confusion matrix:\n",
      "[[160   7  10   0   0   0]\n",
      " [  8 137   4   0   0   0]\n",
      " [  4   2 110   0   0   0]\n",
      " [  0   0   0 169   5   0]\n",
      " [  0   2   0   5 174   0]\n",
      " [  0   0   0   0   0 196]]\n"
     ]
    }
   ],
   "source": [
    "#Decision Tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "dtc= DecisionTreeClassifier()\n",
    "dtc.fit(x_train,y_train)\n",
    "y_pred = dtc.predict(x_cv)\n",
    "acc_dtc = round(accuracy_score(y_pred, y_cv) * 100, 2)\n",
    "print(acc_dtc)\n",
    "#confusion matrix: \n",
    "matrix = confusion_matrix(y_pred, y_cv)\n",
    "print(\"confusion matrix:\")\n",
    "print(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95.77\n",
      "confusion matrix:\n",
      "[[160   2  12   0   0   0]\n",
      " [  6 142   2   0   1   0]\n",
      " [  6   4 110   0   0   0]\n",
      " [  0   0   0 170   5   0]\n",
      " [  0   0   0   4 173   0]\n",
      " [  0   0   0   0   0 196]]\n"
     ]
    }
   ],
   "source": [
    "#LR\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "lrc= LogisticRegression()\n",
    "dtc.fit(x_train,y_train)\n",
    "y_pred = dtc.predict(x_cv)\n",
    "acc_lr = round(accuracy_score(y_pred, y_cv) * 100, 2)\n",
    "print(acc_lr)\n",
    "#confusion matrix: \n",
    "matrix = confusion_matrix(y_pred, y_cv)\n",
    "print(\"confusion matrix:\")\n",
    "print(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95.97\n",
      "confusion matrix:\n",
      "[[162   2  11   0   0   0]\n",
      " [  6 142   2   0   1   0]\n",
      " [  4   4 111   0   0   0]\n",
      " [  0   0   0 170   6   0]\n",
      " [  0   0   0   4 172   0]\n",
      " [  0   0   0   0   0 196]]\n"
     ]
    }
   ],
   "source": [
    "#LDA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "ldac= LinearDiscriminantAnalysis()\n",
    "dtc.fit(x_train,y_train)\n",
    "y_pred = dtc.predict(x_cv)\n",
    "acc_lda = round(accuracy_score(y_pred, y_cv) * 100, 2)\n",
    "print(acc_lda)\n",
    "#confusion matrix: \n",
    "matrix = confusion_matrix(y_pred, y_cv)\n",
    "print(\"confusion matrix:\")\n",
    "print(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Method</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>KNN</td>\n",
       "      <td>96.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LinearDiscriminantAnalysis</td>\n",
       "      <td>95.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>95.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>95.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SVM</td>\n",
       "      <td>93.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaiveBayes</td>\n",
       "      <td>75.93</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Method  Score\n",
       "0                         KNN  96.27\n",
       "5  LinearDiscriminantAnalysis  95.97\n",
       "4          LogisticRegression  95.77\n",
       "3               Decision Tree  95.27\n",
       "1                         SVM  93.66\n",
       "2                  NaiveBayes  75.93"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models = pd.DataFrame({\n",
    "    'Method': ['KNN', 'SVM', \n",
    "              'NaiveBayes', 'Decision Tree', 'LogisticRegression', 'LinearDiscriminantAnalysis'],\n",
    "    'Score': [acc_knn, acc_svc, \n",
    "              acc_nb, acc_dtc, acc_lr, acc_lda]})\n",
    "models.sort_values(by='Score', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QUESTION\n",
    "\n",
    "- How would you deploy the chosen algorithm? What were your considerations in making this decision?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "I'll be using KNN and will check accuracy rate at various values of parameter! I'm using KNN as it has good accuracy rate test at vaiorus split in previous cells.It was almost same for KNN and Decision Tree but KNN is just slightly ahead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code\n",
    "from sklearn.model_selection import cross_val_score\n",
    "myList=list(range(1,50))\n",
    "\n",
    "# subsetting just the odd ones\n",
    "neighbors = filter(lambda x: x % 2 != 0, myList)\n",
    "\n",
    "# empty list that will hold cv scores\n",
    "cv_scores = []\n",
    "\n",
    "# perform 10-fold cross validation\n",
    "for k in neighbors:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    scores = cross_val_score(knn, x_train, y_train, cv=10, scoring='accuracy')\n",
    "    cv_scores.append(scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Checking for the best K.\n",
    "index=list(range(1,50,2))\n",
    "# changing to misclassification error\n",
    "MSE = [1 - x for x in cv_scores]\n",
    "# plot misclassification error vs k\n",
    "plt.plot(index, MSE)\n",
    "plt.xlabel('Number of Neighbors K')\n",
    "plt.ylabel('Misclassification Error')\n",
    "plt.show()\n",
    "#Selecting K=3 as it has lowest error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5. 5. 5. ... 6. 6. 6.]\n"
     ]
    }
   ],
   "source": [
    "#Applying KNN to the test set!\n",
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "knn.fit(x_train, y_train)\n",
    "y_pred = knn.predict(test_data1.drop('ID',axis=1))\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QUESTION\n",
    "\n",
    "- What are your overall conclusions to the problem? What have you made available and how can we use what was built?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "We have just predicted the activity of test dataset by using our KNN model.\n",
    "Conclusions:\n",
    "\n",
    "  1.We can predict the activity of user by just having few significant features as it is provinding a good accuracy      rate.\n",
    "  \n",
    "  2.KNN model with clusters of 3 seems suitable to solve this problem as it is having lowest Misclassification Error\n",
    "  \n",
    " **Availability**\n",
    " \n",
    " Now I'm creating a separate CSV which will be having the data of test_data and an addition column of 'activiy'        which we have just predicted. \n",
    " \n",
    " **Usability**\n",
    " \n",
    " You can use this model to predict the user activity on the basis of their features\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data['activity'] = y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "inaldf = pd.DataFrame(test_data)\n",
    "inaldf.to_csv('predictions.csv', index=False, header=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
